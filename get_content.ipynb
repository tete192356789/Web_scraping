{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def get_req(url):\n",
    "    return requests.get(url).text\n",
    "def get_soup(req):\n",
    "    return BeautifulSoup(req,'html.parser')\n",
    "       \n",
    "all_url = []\n",
    "def crawl(url):\n",
    "   \n",
    "    req = get_req(url)\n",
    "    soup = get_soup(req)\n",
    "      \n",
    "    for i in soup.find_all('a'):\n",
    "        \n",
    "        try:\n",
    "            if i['href'].startswith('/'):\n",
    "                link = urljoin(url,i.get('href'))\n",
    "                all_url.append(link)\n",
    "               \n",
    "            elif i['href'].startswith('h'):\n",
    "                link = i.get('href')\n",
    "                all_url.append(link)\n",
    "              \n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "   \n",
    "   \n",
    "crawl('https://www.mmorpg.com/')\n",
    "print(len(all_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a780a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b008a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "patt = re.compile(r'(https://www.gamespot.com/(reviews|videos|articles)/)')\n",
    "new_list = list(filter(patt.match,all_url))\n",
    "for i in new_list:\n",
    "    req = get_req(i)\n",
    "    soup = get_soup(req)\n",
    "    for k in soup.find_all('section',{'class':'article-body typography-format'}):\n",
    "        for p in k.find_all('p'):\n",
    "            p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0d7e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Kirby And The Forgotten Land Review - The Best Kirby Yet\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "url2 ='https://www.gamespot.com/reviews/kirby-and-the-forgotten-land-review-the-best-kirby-yet/1900-6417850/'\n",
    "req = get_req(url2)\n",
    "soup = get_soup(req)\n",
    "for k in soup.find_all('h1'):\n",
    "    print(k.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070f5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentences\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "pol = []\n",
    "for i in soup.find_all('p'):\n",
    "    sens = i.text\n",
    "    tb =TextBlob(sens,analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "    pol.append(tb.p_pos)\n",
    "\n",
    "final_pol = sum(pol)\n",
    "if final_pol > 0 :\n",
    "    print('positive sentences')\n",
    "elif final_pol == 0 :\n",
    "    print('netural sentences')\n",
    "else:\n",
    "    print('negative sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_sentiment(sens_list):\n",
    "    pol = []\n",
    "    for sen in sens_list\n",
    "        tb = TextBlob(sen,analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "        pol.append(tb.p_pos)\n",
    "        \n",
    "    final_pol = sum(pol)\n",
    "    if final_pol > 0 :\n",
    "        res = 'Positive'\n",
    "    elif final_pol == 0 :\n",
    "        res = 'Neutral'\n",
    "    else:\n",
    "        res = 'Negative'\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_spot_content(all_url):\n",
    "    data = []\n",
    "    pattern =  re.compile(r'(https://www.gamespot.com/(reviews|videos|articles)/)')\n",
    "    new_url = list(filter(pattern.match,all_url))\n",
    "    \n",
    "    for i in new_url:\n",
    "        obj = {}\n",
    "        text =  ''\n",
    "        req = get_req(i)\n",
    "        soup = get_soup(req)\n",
    "        \n",
    "        obj['name'] = i.replace('https://www.','').split('.')[0]\n",
    "        \n",
    "        obj['url'] = i \n",
    "\n",
    "        for header in soup.find_all('h1'):\n",
    "            obj['header'] = header.text.replace('\\n','').replace('\\ ','').strip()\n",
    "\n",
    "        for k in soup.find_all('section',{'class': re.compile(r'(article-body typography-format)')}):\n",
    "            for p in k.find_all('p'):\n",
    "                text = text + p.get_text().replace('\\n','').replace('\\ ','').strip()\n",
    "        if text == '':\n",
    "            obj['content'] = 'No Content.'\n",
    "        else:\n",
    "            obj['content'] = text\n",
    "        data.append(obj)\n",
    "    return data\n",
    "\n",
    "def game_informer_content(all_url):\n",
    "    data = []\n",
    "    pattern =  re.compile(r'(https://www.gameinformer.com/(\\d+/\\d+/\\d+|gamer-culture|sweepstakes)/)')\n",
    "    new_url = list(filter(pattern.match,all_url))\n",
    "    \n",
    "    for i in new_url:\n",
    "        obj = {}\n",
    "        text =  ''\n",
    "        req = get_req(i)\n",
    "        soup = get_soup(req)\n",
    "        \n",
    "        obj['name'] = i.replace('https://www.','').split('.')[0]\n",
    "\n",
    "        obj['url'] = i \n",
    "\n",
    "        for header in soup.find_all('h1'):\n",
    "            obj['header'] = header.text.replace('\\n','').replace('\\ ','').strip()\n",
    "\n",
    "        for k in soup.find_all('div',{'class': re.compile(r'(clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden gi5-body gi5-text-with-summary field__item)')}):\n",
    "            for p in k.find_all('p'):\n",
    "                text = text + p.get_text().replace('\\n','').replace('\\ ','').strip()\n",
    "        if text == '':\n",
    "            obj['content'] = 'No Content.'\n",
    "        else:\n",
    "            obj['content'] = text\n",
    "        data.append(obj)\n",
    "    return data\n",
    "\n",
    "def n4g_content(all_url):\n",
    "    data = []\n",
    "    pattern =  re.compile(r'(https://n4g.com/(news)/)')\n",
    "    new_url = list(filter(pattern.match,all_url))\n",
    "    \n",
    "    for i in new_url:\n",
    "        obj = {}\n",
    "        text =  ''\n",
    "        req = get_req(i)\n",
    "        soup = get_soup(req)\n",
    "        \n",
    "        obj['name'] = i.replace('https://www.','').split('.')[0]\n",
    "\n",
    "        obj['url'] = i \n",
    "\n",
    "        for header in soup.find_all('h1'):\n",
    "            obj['header'] = header.text.replace('\\n','').replace('\\ ','').strip()\n",
    "\n",
    "        for k in soup.find_all('div',{'class': re.compile(r'(sd-content)')}):\n",
    "            for p in k.find_all('p'):\n",
    "                text = text + p.get_text().replace('\\n','').replace('\\ ','').strip()\n",
    "        if text == '':\n",
    "            obj['content'] = 'No Content.'\n",
    "        else:\n",
    "            obj['content'] = text\n",
    "        data.append(obj)\n",
    "    return data\n",
    "\n",
    "def gematsu_content(all_url):\n",
    "    data = []\n",
    "    pattern =  re.compile(r'(https://www.gematsu.com/\\d{4}/\\d{2}/)')\n",
    "    new_url = list(filter(pattern.match,all_url))\n",
    "    \n",
    "    for i in new_url:\n",
    "        obj = {}\n",
    "        text =  ''\n",
    "        req = get_req(i)\n",
    "        soup = get_soup(req)\n",
    "        \n",
    "        obj['name'] = i.replace('https://www.','').split('.')[0]\n",
    "\n",
    "        obj['url'] = i \n",
    "\n",
    "        for header in soup.find_all('h1'):\n",
    "            obj['header'] = header.text.replace('\\n','').replace('\\ ','').strip()\n",
    "\n",
    "        for k in soup.find_all('div',{'class': re.compile(r'(post__content-body advert__autofill_content)')}):\n",
    "            for p in k.find_all('p'):\n",
    "                text = text + p.get_text().replace('\\n','').replace('\\ ','').strip()\n",
    "        if text == '':\n",
    "            obj['content'] = 'No Content.'\n",
    "        else:\n",
    "            obj['content'] = text\n",
    "        data.append(obj)\n",
    "    return data\n",
    "\n",
    "def techradar_content(all_url):\n",
    "    data = []\n",
    "    pattern =  re.compile(r'(https://www.techradar.com/(news)/)')\n",
    "    new_url = list(filter(pattern.match,all_url))\n",
    "    \n",
    "    for i in new_url:\n",
    "        obj = {}\n",
    "        text =  ''\n",
    "        req = get_req(i)\n",
    "        soup = get_soup(req)\n",
    "        \n",
    "        obj['name'] = i.replace('https://www.','').split('.')[0]\n",
    "\n",
    "        obj['url'] = i \n",
    "\n",
    "        for header in soup.find_all('h1'):\n",
    "            obj['header'] = header.text.replace('\\n','').replace('\\ ','').strip()\n",
    "\n",
    "        for k in soup.find_all('div',{'id': re.compile(r'(article-body)')}):\n",
    "            for p in k.find_all('p'):\n",
    "                text = text + p.get_text().replace('\\n','').replace('\\ ','').strip()\n",
    "        if text == '':\n",
    "            obj['content'] = 'No Content.'\n",
    "        else:\n",
    "            obj['content'] = text\n",
    "        data.append(obj)\n",
    "    return data\n",
    "\n",
    "def mmorpg_content(all_url):\n",
    "    data = []\n",
    "    pattern =  re.compile(r'(https://www.mmorpg.com/(news|reviews|videos)/)')\n",
    "    new_url = list(filter(pattern.match,all_url))\n",
    "    \n",
    "    for i in new_url:\n",
    "        obj = {}\n",
    "        text =  ''\n",
    "        req = get_req(i)\n",
    "        soup = get_soup(req)\n",
    "        \n",
    "        obj['name'] = i.replace('https://www.','').split('.')[0]\n",
    "\n",
    "        obj['url'] = i \n",
    "\n",
    "        for header in soup.find_all('h1'):\n",
    "            obj['header'] = header.text.replace('\\n','').replace('\\ ','').strip()\n",
    "\n",
    "        for k in soup.find_all('div',{'class': re.compile(r'(post_content)')}):\n",
    "            for p in k.find_all('p'):\n",
    "                text = text + p.get_text().replace('\\n','').replace('\\ ','').strip()\n",
    "        if text == '':\n",
    "            obj['content'] = 'No Content.'\n",
    "        else:\n",
    "            obj['content'] = text\n",
    "        data.append(obj)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84498e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_url = crawl('https://www.gamespot.com/')\n",
    "a = game_spot_content(all_url)\n",
    "df = pd.DataFrame.from_dict(a)\n",
    "\n",
    "all_url2 = crawl('https://www.gameinformer.com/')\n",
    "a2 = game_informer_content(all_url2)\n",
    "df2 = pd.DataFrame.from_dict(a2)\n",
    "\n",
    "all_url3 = crawl('https://www.gematsu.com/')\n",
    "a3 = gematsu_content(all_url3)\n",
    "df3 = pd.DataFrame.from_dict(a3)\n",
    "\n",
    "all_url4 = crawl('https://www.techradar.com/')\n",
    "a4 = techradar_content(all_url4)\n",
    "df4 = pd.DataFrame.from_dict(a4)\n",
    "\n",
    "all_url5 = crawl('https://n4g.com/')\n",
    "a5 = techradar_content(all_url5)\n",
    "df5 = pd.DataFrame.from_dict(a5)\n",
    "\n",
    "all_url6 = crawl('https://www.mmorpg.com/')\n",
    "a6 = techradar_content(all_url6)\n",
    "df6 = pd.DataFrame.from_dict(a6)\n",
    "\n",
    "sum_df = pd.concat([df, df2,df3,df4,df5,df6], axis=0,ignore_index = True)\n",
    "sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885994a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df.to_csv('content.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f482310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_req(url):\n",
    "    return requests.get(url).text\n",
    "def get_soup(req):\n",
    "    return BeautifulSoup(req,'html.parser')\n",
    "\n",
    "def crawl5(url):\n",
    "    l = []\n",
    "    req = get_req(url)\n",
    "    soup = get_soup(req)\n",
    "        \n",
    "    for i in soup.find_all('a'):\n",
    "      \n",
    "        try:\n",
    "            if i['href'].startswith('/'):\n",
    "                link = urljoin(url,i['href'])\n",
    "               \n",
    "\n",
    "                l.append(link)\n",
    "                l2 =crawl(link)\n",
    "            elif i['href'].startswith('h'):\n",
    "                link = i['href']\n",
    "               \n",
    "                l.append(link)\n",
    "                l2 = crawl(link)\n",
    "            else:\n",
    "                pass\n",
    "            l = l + l2\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ced21",
   "metadata": {},
   "outputs": [],
   "source": [
    "allu = crawl5('https://www.techradar.com/')\n",
    "len(allu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d73a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allu"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2d82fc6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "215e8a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '#main': No scheme supplied. Perhaps you meant http://#main?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6388/795867426.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mpages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mgetLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mgetLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.techradar.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6388/795867426.py\u001b[0m in \u001b[0;36mgetLinks\u001b[1;34m(pageUrl)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mpages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mgetLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mgetLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.techradar.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6388/795867426.py\u001b[0m in \u001b[0;36mgetLinks\u001b[1;34m(pageUrl)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_req\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6388/365758923.py\u001b[0m in \u001b[0;36mget_req\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_req\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         )\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '#main': No scheme supplied. Perhaps you meant http://#main?"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "pages = []\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = get_req(pageUrl)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    for link in bs.find_all('a'):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                \n",
    "                pages.append(newPage)\n",
    "                getLinks(newPage)\n",
    "getLinks('https://www.techradar.com/')\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f11fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
